--- 
title: "TMA4315 Generalized Linear Models" 
subtitle: "Compulsory exercise 2: Logistic regression and Poisson regression" 
author: "Liv Elise Herstad and Julie Berg"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  # html_document
  pdf_document

---

```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)


```


# Part 1: Logistic regression


Wikipedia's *List of highest mountains* (https://en.wikipedia.org/wiki/List_of_highest_mountains_on_Earth) lists 118 of the world's highest mountains, along with some properties of each one, including the number of successful and failed attempts at reaching the summit as of 2004. In this problem, we will consider a data set consisting of the height (in meters), topographic prominence (also in meters), number of successful ascents and number of failed attempts for 113 of the mountains on the list. The mountains Mount Everest (height 8848, prominence 8848), Muztagh Ata, Ismoil Somoni Peak, and Jengish Chokusu/.../Pk Pobeda are excluded from the dataset because of incomplete data. In addition the mountain Chogolisa (height 7665, prominence 1624) is removed from the data set to be used for prediction.

## a) 
\textit{Fit a glm modelling how the probability that an attempt at reaching a particular summit depends on its height and prominence. Introduce necessary mathematical notation and give a brief summary of the mathematical assumption of the model and discuss if these assumptions seem reasonable. Also explain the rationale behind your choice of link function.}

$y_i$: number of successful ascents for $i$th mountain
$n_i$: total number of trials (successful ascend + failure) for $i$th mountain

We choose binary regression because the response variable $Y_i$ is binomially distributed and categorical. 
The choice of an appropriate link function is dependent on the type of response variable. Since we are dealing with a binary response model, we use the logit link function, obtaining the Logit model.
The logit link is the natural log of the odds that $Y_i$ take on one of the categories, and forms a somewhat linear relationship with the predictors. 

Our model choice for the probability of success is $Y_i \sim \text{Bin}(n_i, \pi_i) \quad \text{for} \  i = 1,\ldots,113$ with linear predictor $\eta_i = \mathbf{x}_i^T\boldsymbol{\beta}$ and link function $\ln \left(\frac{\pi_i}{1-\pi_i} \right)$, where $\mathbf{x}_i$ is the vector consisting of the covariates 'height' and 'prominence' for the $i$th observation, and $\boldsymbol{\beta}$ is the vector of regression parameters. 

The model then looks like this:
$$
\ln \left(\frac{\pi_i}{1-\pi_i} \right) = \eta = \beta_0 + \beta_1x_1 + \beta_2x_2
$$
An increase in one unit of one of the predictors (while keeping the other one constant) would result in the log of the odds to increase by the respective parameter. To get a better understanding about the effect of the predictor, it is wise to take the exponent of the linear predictor, like this:

$$
\frac{\pi_i}{1-\pi_i} = \exp(\eta) = \exp(\beta_0)\exp(\beta_1x_1)\exp(\beta_2x_2)
$$
We get the probability of $Y_i = 1$, i.e. the probability of succeeding in climbing mountain $i$ by 

$$
P(Y_i = 1) = \pi_i = \frac{\exp{(\mathbf{x}_i^T\boldsymbol{\beta})}}{1+\exp{(\mathbf{x}_i^T\boldsymbol{\beta})}}
$$

See code for the fitted glm model

```{r, eval=TRUE, echo=TRUE}
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
mount <- read.table(file = filepath, header = TRUE, col.names = c("height", "prominence", "fail", "success"))
attach(mount) 
model1 <- glm(cbind(success, fail) ~ height + prominence, data = mount, family = "binomial")
summary(model1) #AIC = 686.03
```


## b) 
\textit{Based on the observed deviance, is there any evidence of overdispersion in the data? What are potential sources of overdispersion in the data? If necessary, refit a model using a quasi-likelihood model.}

Overdispersion is the presence of greater variability in a data sat than would be expected based on a statistical model. 

Checking for overdispersion by doing a Chi-squared test, since $D \sim \chi^2_{110}$. There are signs of overdispersion if the deviance, D, of a model is greater than the $\chi^2$ with $110$ degrees of freedom. 
```{r, eval=TRUE, echo=TRUE}
deviance <- summary(model1)$deviance
deviance
degfree1 <- summary(model1)$df.residual
chisquared <- qchisq(1-0.05, df=113-3) # chi-squared = 135.4802
chisquared
```

As ... we see that there is overdispersion in the data. One can also divide the deviance, D, by the number of degrees of freedom, and if the number is close to 1 there is no or little to no sign of overdispersion. 

```{r, eval=TRUE, echo=TRUE}
deviance/110
```

Since $\frac{D}{110} \not\approx 1$ our second test also suggest that there is overdispersion in the data. 

Potential sources for overdispersion can, in general, be unobserved heterogeneity and positive correlation between individual binary observations of $Y$. For our data, there is quite possibly a lot of heterogeneity, meaning there are many differences in our sample. When people are climbing mountains with different heights and different prominences, there is bound to be some differences in what different people are able to do. There could also be some correlation between the individual binary observations, as people tend to climb mountains in groups. If one person has to stop because of different reasons (maybe lack of fitness or simply an injury), there is a big chance the entire group quits or atleast some quit. 

Whatever the reasons, we do have overdispersion in our data and we therefore fit a Quasibinomial model instead of a Binomial model to the data, and make a new glm modelling the probability of reaching a mountain's summit. 

```{r, eval=TRUE, echo=TRUE}
model2 <- glm(cbind(success, fail) ~ height + prominence, data = mount, family = "quasibinomial")
summary(model2) #no AIC
```

As we can see from the summary of the model, there is no QAIC calculated for the Quasibinomial model. The QAIC for this model is calculated under task c). 

## c) 
\textit{Using the ordinary AIC criteria, choose a best model for the data. Burnham and Anderson recommends that $\hat{\phi}$ here should be a common estimate of the overdispersion parameter under the full model (the model including all covariates). Also, $\phi$ counts as one parameter. Using these criteria, decide which covariates you want to include in the model. You'll need to compute QAIC values manually by fitting non-quasi-likelihood models to get $l(\hat{\theta})$ as R doesn't include QAIC by default.}

The AIC is the Akaike Information Criterion, and we can use it to determine which model is the best by chosing the one with lowest AIC. Since we had to fit a Quasibinomial model, we get a QAIC instead of an AIC. However, the glm function in R does not compute it automatically, so the following code shows fitted glm models for each submodel including a manually computed QAIC value. Luckily, it is quite easy to compute the QAIC when one can just extract the log likelihood from each binomial model with the same predictors as the model we are investigating. We use the same dispersion parameter for all of the submodels, but the log likelihod for each model is computed every time a new model is fitted. The QAIC "punishes" larger models just as AIC does, seen from the following formula

$$
QAIC_i = \frac{-2 \cdot l_i(\hat{\beta})}{\hat{\phi}} + 2 \cdot p
$$

where $p$ is the number of parameters included in model $i$, $\beta_0$ and $\phi$ included as parameters. The 'intercept' is included in all of the models, so we will only point out which predictors that differ. 

First we compute the QAIC from the model fitted in b). 

```{r, eval=TRUE, echo=TRUE}
disp <- summary(model2)$dispersion
loglik1 <- logLik(model1) #loglikelihood from binomial model
QAIC_2 <- -2*loglik1/disp +2*4 #p = 4
QAIC_2 #188.0638
```

Here, the QAIC for the model with both 'height' and 'prominence' is `r QAIC_2`.

Next, we fit the glm with just the predictor 'height', calling it model 3.

```{r, eval=TRUE, echo=TRUE}
model3 <- glm(cbind(success, fail) ~ height, data = mount, family = "quasibinomial")
summary(model3) #no AIC

loglik3 <- sum(dbinom(success, success+fail,fitted(model3), log=TRUE))
QAIC_3 <- -2*loglik3/disp +2*3 #p = 3
QAIC_3 #189.9345
```

The QAIC for model 3 is `r QAIC_3`. 

Next up is the model with only 'prominence' as the predictor, calling it model 4. 

```{r, eval=TRUE, echo=TRUE}
model4 <- glm(cbind(success, fail) ~ prominence, data = mount, family = "quasibinomial")
summary(model4) #no AIC

loglik4 <- sum(dbinom(success, success+fail,fitted(model4), log=TRUE))
QAIC_4 <- -2*loglik4/disp +2*3 #p = 3
QAIC_4 #223.0717
```

The QAIC for model 4 is `r QAIC_4`. 

The last glm fitted is the model with only the intercept $\beta_0$, calling it model 5. 

```{r, eval=TRUE, echo=TRUE}
model5 <- glm(cbind(success, fail) ~ 1, data = mount, family = "quasibinomial")
summary(model5) #no AIC

loglik5 <- sum(dbinom(success, success+fail,fitted(model5), log=TRUE))
QAIC_5 <- -2*loglik5/disp +2*2 #p = 2
QAIC_5 #263.66
```

The QAIC is `r QAIC_5`. 

The model with lowest QAIC value is model 2, the model with 'intercept', 'height' and 'prominence' as parameters. 

The QAIC of model 2 is not far from the QAIC of model 3, the model with only 'height' as predictor, suggesting that 'prominence' isn't adding much information. However, it is model 2 that has the lowest QAIC, and we choose this as the best model. 

## d) 
\textit{Also test the significance of each term in the model using both Wald tests and likelihood ratio/F-tests and comment on any differences you see (summary and drop1). Given your choice of link function, give interpretations of the estimated regression slope parameters, in language that you would use to communicate to non-statisticians.}

```{r, eval=TRUE, echo=TRUE}
droppit_F <- drop1(model2, test="F") #quasibinomial F-test
droppit_F
waldqb <- summary(model2) #Wald: p-value from summary
waldqb
```

The significance tests for each term is presented in the R-code above. What we see from the F-test and the Wald test, is that they both agree on 'height' being significant. They also show that the predictor 'prominence' is less significant than predictor 'height'. The p-value for 'prominence' from the F-test is 0.05145 and the p-value from the Wald test is 0.0518, which means it is right above the standard threshold $\alpha = 0.05$ of rejection. 

```{r}
coeffs <- coef(model2) 
coeffs
```

The estimated regression slope parameters is given in the summmary of model 2 under b) as well as being printed in the R-code above.

Since we are working with a glm that's a Quasibinomial model, the coefficients (regression slope parameters) can't be interpreted as we would given a linear model. In the linear model, $y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i}$ the coefficients are easily interpreted as the number one would expect the y-value (response value) to increase by when the respective x-value (predictor) changes by one unit, given that all other x-values (predictors) remain constant. For example if the response value is 'weight of infant' and the predictors are 'age' and 'eye colour', when the infant ages one year and remains its eye color, the expected increase in 'weight' would be the value of the coefficient of predictor 'age'. However, the interpretation isn't as straight forward for our model.

Since the link function chosen was the logit link, the regression model looks like this

$$
\ln \left(\frac{\pi_i}{1-\pi_i} \right) = \eta = \beta_0 + \beta_1x_1 + \beta_2x_2
$$

where the response value is the natural logarithm of the odds, i.e. the natural log of the probability of climbing the mountain divided by the probability of failing. An increase of one unit in one of the predictors would result in the log odds increasing by the coefficient belonging to that predictor. An increase of 'height' by 1 meter would increase the log odds by $\beta_1$ = `r coeffs["height"]`. That increase (or decrease in this case) is very difficult to interpret, so we might rewrite the model so the response variable makes more sense:

$$
\pi_i = \frac{\exp{(\mathbf{x}_i^T\boldsymbol{\beta})}}{1+\exp{(\mathbf{x}_i^T\boldsymbol{\beta})}}
$$

This response is much easier to interpret, as it is the probability of succeeding in climbing mountain $i$. It makes the regression slope parameters less intuitive, but that's the price one has to pay when the data can't be modelled as a linear model.

## e)
\textit{Examine the model fit by plotting the deviance residuals against fitted values and against each covariate (residuals( , type="deviance") and fitted( )).}

```{r}
resid_model2 <- residuals(model2, type="deviance")
fit2 <- fitted(model2)

plot(fit2,resid_model2) #residuals vs. chosen model
plot(height,resid_model2) #residuals vs. covariance height
plot(prominence,resid_model2) #residuals vs. covaiate prominence
```

As the residuals don't show any pattern, we conclude that the Quasibinomial is a suitable fit for the data. 

## f)
The height and (by definition) the prominence of Mount Everest are both 8848 meters. Compute a prediction for the probability that an attempt at this summit will be successful. First consider the predicted value and its variance on the scale of the linear predictor (vcov gives you the estimate variance matrix of $\hat\beta$ ). Also compute a 95% confidence interval for the predicted value on this scale based on asymptotic normality of $\hat\beta$ . Transform this confidence interval to the probability scale and explain the theory behind the transformation you're using.

```{r}
m_height = 8848
m_prom = 8848
newdata = data.frame(m_height, m_prom)
colnames(newdata) = (c("height", "prominence" ))
pred <- predict(model1, newdata)
prob1 <- exp(pred)/(1 + exp(pred))
prob2 <- prob1*100
prob2
```

The predicted probability of success at climbing Mount Everest is `r prob2 `. 

To make a confidence interval for the predicted value, we first concider the distribution of the parameters. We know that
$$
\hat{\beta} \sim N(x^T\beta, F^{-1}(\hat\beta))
$$

However, when the interest is on the predicted value $\eta = x_0^T\hat\beta$, since the parameter is normal, the predicted value is actually normal as well

$$
x_0^T\hat\beta \sim N(x_0^T\beta, x_0^T\beta x_0)
$$

We can then create a confidence interval for the predicted value by making it standard normal by subtracting the mean and dividing by the standard error, with the critical values being standard normal with quantiles $\alpha/2$. 

$$
P(-z_{\alpha/2} \leq \frac{x_0^T\hat\beta-x\beta}{SE(x_0^T\hat\beta)} \leq x_{\alpha/2}) = 1-\alpha
P(x_0^T\hat\beta - SE(x_0^T\hat\beta) \leq x^T\beta \leq x_0^T\hat\beta - SE(x_0^T\hat\beta)) = 1-\alpha
$$

For a 95% confidence interval, $\alpha = 0.05$, making $z_{0.05/2} = 1.96$. Computing this in R results in

```{r}
x0 <- c(1,m_height,m_prom)
variance <- t(x0)%*%vcov(model2)%*%x0
se <- sqrt(variance)
x0beta <- t(x0)%*%coef(model2)
critval = 1.96
zse <- critval*se
A <- x0beta - zse
B <- x0beta + zse
c(A,B)
```

$$
P(`r A` \leq x^T\beta \leq `r B`) = 0.95
$$

The probability of $x_0^T\beta$ being between `r A` and `r B` is 95%. To get the interval on the probability scale, we use the inverse logit function, to get the probabilities back


```{r, eval=TRUE, echo=TRUE}
plogis(A) #inverse logit
plogis(B)
```
As the logit link is monotone, we will get the lower probability by inserting the lowest value and get the highest probability for the highest value. 











\newpage
# Part 2: Eliteserien 2018

In this problem we will use a generalized linear model to analyse part of the 2018 results from the Norwegian elite football league.

Each match is represented by pairs of consecutive rows in the data frame; the first row below contains the number of goals scored by Molde against Sandefjord fotball with Molde playing at their home field, and the second row contains the number of goals by Sandefjord fotball (not playing at their home field) against Molde in the same match. The covariates attach and defence will by default encoded as factors, both with 16 levels and with treatment contrasts and with BodoeGlimt by default chosen as the reference category. The variable 'home' will similarly be encoded as a factor with two levels. 

The data is incomplete in that the number of goals for some matches were not available at the time the data were recorded. Our aim is to fit a model to the available data and then simulate the remaining matches using the fitted model. Based on these simulations we will estimate the probabilities of each team winning the whole league. 

```{r}
long <- read.csv("https://www.math.ntnu.no/emner/TMA4315/2019h/eliteserie.csv")
```

## a)
```{r}
mod <- glm(goals ~ attack + defence + home, poisson, data=long)
```

The assumption of Poisson regression fits well, as the observations $y_i \in \{1, 2, \dots,32 \}$ are count data, indicating how often some event of interest has been observed in a certain period of time. The event of interest is scoring a goal, and the period of time is the 90 minute football game. In a Poisson model it is assumed that the $y_i$ are independently $\text{Po}(\lambda_i)$ distributed, which is reasonable to assume. 

For this model, the response variable is number of goals per game for each team. Reading from the data, there are 16 football teams. The covariates in this model are 'attack', 'defence' and 'home', where 'attack' and 'defence' has the 16 football teams as levels, and 'home' has two levels (yes or no). 

The level 'BodoeGlimt' is chosen as the reference category, meaning that it is included in the intercept.

Given the chosen Poisson regression, the linear predictor $\eta_i =\textbf{x}_i^T\boldsymbol{\beta}$ is connected with the rate $\lambda_i$ via 
$$\lambda_i = \text{exp}(\eta_i) = \text{exp}(\beta_0 + \beta_1x_{i1} + \dots + \beta_{32}x_{i32})$$ 
where $\lambda_i$ is the expected number of goals scored by the chosen team. This can be written in log-linear form:
$$
\ln\lambda_i = \eta_i = \beta_0 + \beta_1x_{i1} + \dots + \beta_{32}x_{i32}
$$ 
and more neatly written as 

$$
\ln\lambda_i = \mu + \alpha_{j(i)} + \beta_{k(i)} + \gamma_{h(i)}
$$
where $\mu$ is the intercept, $\alpha_{j(i)}$ is the attack coefficient for team j in observation i, $\beta_{k(i)}$ is the defence coefficient for team k $\neq$ j in observation i, and $\gamma_{h(i)}$ is the home coefficient dependent on which team is playing their home field (i.e. h = j or h = k). 

The parameters associated with each level of 'attack', 'defence' and 'home' say something about the increase or decrease of the log-response. 'attack' has 16 levels, where each parameter indicates if that team's attack parameter will increase or decrease the log-response if included. Same goes for 'defence', but it is a bit trickier to think about. The lower defence parameter, the better the team. That's because the Poisson model calculates the expected number of goals for one team. That means the attack parameter for that team is included, home parameter is included and the defence parameter of the opposing team is inlcuded as well. If the opposing team's defence parameter is small (or negative) it will not add much to the expected number of goals in the end (it might even subtract). That's why a higher value of attack parameter indicates more goals for the team, and a smaller value of defence parameter indicates a tougher opponent team. 

## b) 
\textit{If good teams tend to have both strong defence and strong attack capabilities, what kind of association would you expect between the above parameters? Is there any evidence of such a relationship between the parameters? You may want to use cor.test to test for this.}

If a team has strong attack capabilities, the coefficient for 'attack' of the team will be large, and if they have strong defence capabilities the 'defence' parameter will be small (or negative). 

Correlation measures the tendency for the variables to fluctuate. The value of the correlation coefficient is between $-1$ and $+1$. In general, a positive correlation will indicate that an increase (decrease) in one variable leads to an increase in the second variable, whereas a negative correlation indicates that an increase in one variable leads to a decrease in the other. From the explanation in a), we would expect a negative correlation if a team has good attack capabilities (large attack coefficient) and good defence capabilites (small or negative defence coefficient).

We calculate the correlation between the parameters attack and defence, and show the result below.

```{r}
correlation <- cor.test(mod$coefficients[2:16] , mod$coefficients[17:31] , data=long)
corr <- correlation$estimate
```

As you can see, the correlation is `r corr`, which is a medium negative correlation value on the scale from [-1,0]. This supports what we expected. 

## c) 
\textit{Based on the observed deviance, is there evidence of overdispersion in the data? What would be possible sources of over- and underdispersion in this example?}

To check for overdispersion in the data we use the trusted Chi squared test: if the deviance is larger than the Chi squared 0.05 and 352 degrees of freedom quantile, there is evidence of overdispersion in the model.

The calculations are shown below:

```{r}
d_mod2 <- summary(mod)$deviance
d_mod2
chisq_mod2 <- qchisq(1-0.05, df=352)
chisq_mod2

# Alternative method:
d_mod2 / 352 #divide deviance by df, if close to 1 --> not overdispersion
```

Since $\text{D} \ngtr \chi^2_{0.05,352}$, we conclude that there is no overdispersion in the data, and we do not need to introduce a quasipoisson model. The alternative method of dividing the deviance by the degrees of freedom is close to one, indicating that there indeed is no sign of overdispersion. 

Reasons for over- or underdispersion are once again, unobserved heterogeneity and positive correlation between the responses. When team A and team B play on team A's home field, we could expect some correlation when the same teams play on team B's home field. That way we would get correlation between goals scored based on whose home field they're playing. There could also be some unobserved heterogeneity because there are some unmeasured differences between the teams, that aren't represented by the included variables. 


## d) 
```{r}
tournamentPoints <- function(tournament) { #takes in a data frame
  tournament <- na.omit(tournament)
  teamnames <- c(levels(long$attack))
  teams <- c(rep(0, 16))
  names(teams) <-teamnames
  difference <- c(rep(0, 16))
  total <- c(rep(0, 16))
  names(difference) <- teamnames
  names(total) <- teamnames
  for(i in 1:(nrow(tournament))) { 
    difference[tournament$attack[i]] = difference[tournament$attack[i]] + tournament$goals[i]
    difference[tournament$defence[i]] = difference[tournament$defence[i]] - tournament$goals[i]
    
    total[tournament$attack[i]] = total[tournament$attack[i]] + tournament$goals[i]
  }
  
  for(i in 1:(nrow(tournament)/2)) { 
    if(tournament$goals[2*i] > tournament$goals[2*i-1]) {
      teams[tournament$attack[2*i]] = teams[tournament$attack[2*i]] + 3
    } 
    else if(tournament$goals[2*i] < tournament$goals[2*i-1]) {
      teams[tournament$attack[2*i-1]] = teams[tournament$attack[2*i-1]] + 3
    }
    
    else {
      teams[tournament$attack[2*i-1]] = teams[tournament$attack[2*i-1]] + 1
      teams[tournament$attack[2*i]] = teams[tournament$attack[2*i]] + 1
    }
  }
  
  #the thing to sort
  test <- data.frame(teams, difference, total)
  ranks <- data.table::frankv(test, ties.method=c("random"))
  
  ranking <- vector()
  
  for(i in 16:1) {
    index <- match(i, ranks)
    ranking[17-i] <- teamnames[index]
  }
  return(ranking)
}

teams <- tournamentPoints(long)
print(teams)
```

##e)
Expected number of goals
```{r}
#predict(mod, newdata, type="response")
```

##f)
\textit{A possible simplification of the model would be to assume the attack and defence strengths of each team are equal in absolute value but of opposite sign. Explain how this is a linear hypothesis on the form $C\beta = d$. Perform a Wald test of this hypothesis. The estimate of the variance matrix of $\hat\beta$ is available via the 'vcov()' function.}

A way to write that attack and defence strengths are equal in absolute value but of opposite sign is
$$
\beta_i = -\beta_{i+15}
$$

To check every coefficient for $i=1,\dots, 15$ we can write $C\beta = d$, where

\[
C = 
\begin{bmatrix}
    0 & 1 & 0 & \dots & 0 & 1 & 0 & \dots & 0 & 0\\
    0 & 0 & 1 & \dots & 0 & 0 & 1 & \dots & 0 & 0  \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \ddots  & \vdots  & \vdots\\
    0 & 0 & 0 & \dots & 1 & 0 & 0 & \dots & 1 & 0
\end{bmatrix}
, 
\beta =
\begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \vdots \\
    \beta_{30} \\
    \beta_{31}
\end{bmatrix}
, 
d =
\begin{bmatrix}
    0 \\
    0 \\
    \vdots \\
    0 \\
    0 
\end{bmatrix}
\]


where C is the matrix of dimension 15 $\times$ 32, $\beta$ has dimension 32 $\times$ 1 and d has dimension 15 $\times$ 1. 

```{r, eval=TRUE, echo=TRUE}
b <- mod$coefficients #the betas
C <- matrix(0, nrow = 15, ncol = 32)
for (i in 1:15){
  #beta1 has index 2 as so on
  C[i,i+1] <- 1
  C[i,i+1+15] <- 1 
}
library(Matrix)
r <- rankMatrix(C)[1] #rank of matrix C
d <- rep(0,17) #d vector (we dont rly use this because why subtract 0)
V <- vcov(mod) #covariance matrix for coefficients
library(MASS) #needed for solve function (inverse)
W <- t(C%*%b)%*%solve(C%*%(V)%*%t(C))%*%(C%*%b) #Wald test for linear hypotheses
W #Wald test value
chisq <- qchisq(1-0.05, df=r) #chi squared quantile for r = 15
chisq #Chi squared value
```

The Wald test value is `r W`, and since the Wald test is calculated using squared Normal distributed variables, $W \sim \chi^2_{r}$ where $r$ is the rank of the matrix C. We can therefore check the Wald test value against a chi squared quantile, and reject the null hypothesis if $W > \chi^2_{r}$. The $\chi^2_r$ value is `r chisq`, and therefore we do not reject the null hypothesis, and we can simplify the model by the assumptions described earlier. 
