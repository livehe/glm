--- 
title: "TMA4315 Generalized Linear Models" 
subtitle: "Compulsory exercise 2: Logistic regression and Poisson regression" 
author: "Liv Elise Herstad and Julie Berg"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  # html_document
  pdf_document

---

```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)


```


# Part 1: Logistic regression


Wikipedia's *List of highest mountains* (https://en.wikipedia.org/wiki/List_of_highest_mountains_on_Earth) lists 118 of the world's highest mountains, along with some properties of each one, including the number of successful and failed attempts at reaching the summit as of 2004. In this problem, we will consider a data set consisting of the height (in meters), topographic prominence (also in meters), number of successful ascents and number of failed attempts for 113 of the mountains on the list. The mountains Mount Everest (height 8848, prominence 8848), Muztagh Ata, Ismoil Somoni Peak, and Jengish Chokusu/.../Pk Pobeda are excluded from the dataset because of incomplete data. In addition the mountain Chogolisa (height 7665, prominence 1624) is removed from the data set to be used for prediction.

## a) 
Fit a glm modelling how the probability that an attempt at reaching a particular summit depends on its height and prominence. Introduce necessary mathematical notation and give a brief summary of the mathematical assumption of the model and discuss if these assumptions seem reasonable. Also explain the rationale behind your choice of link function.

$y_i$: number of successful ascents for $i$th mountain
$n_i$: total number of trials (successful ascend + failure) for $i$th mountain

We choose binary regression because the response variable $Y_i$ is binomially distributed and categorical. For link function we choose logit link as it is the natural log of the odds that $Y_i$ take on one of the categories, and forms a somewhat linear relationship with the variables. 

Our model choice for the probability of success is $Y_i \sim \text{Bin}(n_i, \pi_i) \quad \text{for} \  i = 1,\ldots,113$ with linear predictor $\eta_i = \mathbf{x}_i^T\boldsymbol{\beta}$ and link function $\ln \left(\frac{\pi_i}{1-\pi_i} \right)$, where $\mathbf{x}_i$ is the vector consisting of the covariates for the $i$th observation, and $\boldsymbol{\beta}$ is the vector of regression parameters. 

```{r}
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
mount <- read.table(file = filepath, header = TRUE, col.names = c("height", 
                                            "prominence", "fail", "success"))
attach(mount) 
model1 <- glm(cbind(success, fail) ~ height + prominence, data = mount, family = "binomial")
summary(model1) #AIC = 686.03
```


## b) 
Based on the observed deviance, is there any evidence of overdispersion in the data? What are potential sources of overdispersion in the data? If necessary, refit a model using a quasi-likelihood model.

```{r}
deviance <- summary(model1)$deviance
deviance
degfree1 <- summary(model1)$df.residual
chisquared <- qchisq(1-0.05, df=113-3) # chi-squared = 135.4802
chisquared
```

```{r}
model2 <- glm(cbind(success, fail) ~ height + prominence, data = mount, family = "quasibinomial")
summary(model2) #no AIC
```


## c) 
Using the ordinary AIC criteria, choose a best model for the data. Burnham and Anderson recommends that $\hat{\phi}$ here should be a common estimate of the overdispersion parameter under the full model (the model including all covariates). Also, $\phi$ counts as one parameter. Using these criteria, decide which covariates you want to include in the model. You'll need to compute QAIC values manually by fitting non-quasi-likelihood models to get $l(\hat{\theta})$ as R doesn't include QAIC by default.

```{r}
disp <- summary(model2)$dispersion
loglik1 <- logLik(model1)
QAIC_2 <- -2*loglik1/disp +2*4 #p = 4
QAIC_2 #188.0638
```

```{r}
model3 <- glm(cbind(success, fail) ~ height, data = mount, family = "quasibinomial")
summary(model3) #no AIC

loglik3 <- sum(dbinom(success, success+fail,fitted(model3), log=TRUE))
QAIC_3 <- -2*loglik3/disp +2*3 #p = 3
QAIC_3 #189.9345
```

```{r}
model4 <- glm(cbind(success, fail) ~ prominence, data = mount, family = "quasibinomial")
summary(model4) #no AIC

loglik4 <- sum(dbinom(success, success+fail,fitted(model4), log=TRUE))
QAIC_4 <- -2*loglik4/disp +2*3 #p = 3
QAIC_4 #223.0717
```

```{r}
model5 <- glm(cbind(success, fail) ~ 1, data = mount, family = "quasibinomial")
summary(model5) #no AIC

loglik5 <- sum(dbinom(success, success+fail,fitted(model5), log=TRUE))
QAIC_5 <- -2*loglik5/disp +2*2 #p = 2
QAIC_5 #263.66
```


## d) 
Also test the significance of each term in the model using both Wald tests and likelihood ratio/F-tests and comment on any differences you see (summary and drop1). Given your choice of link function, give interpretations of the estimated regression slope parameters, in language that you would use to communicate to non-statisticians.

```{r, eval=TRUE, echo=FALSE}
droppit_F <- drop1(model2, test="F") #quasibinomial F-test
droppit_F
waldqb <- summary(model2) #Wald: p-value from summary
waldqb
```

The significance tests for each term is presented in the R-code above. We see from both the F-test and the Wald test that the covariate 'prominence' is less significant than covariate 'height'. The p-value from the F-test is 0.05145 and the p-value from the Wald test is 0.0518, which means it is right above the standard threshold $\alpha = 0.05$ of rejection. 

## e)
Examine the model fit by plotting the deviance residuals against fitted values and against each covariate (residuals( , type="deviance") and fitted( )).

```{r}
resid_model2 <- residuals(model2, type="deviance")
fit2 <- fitted(model2)

plot(fit2,resid_model2) #residuals vs. chosen model
plot(height,resid_model2) #residuals vs. covariance height
plot(prominence,resid_model2) #residuals vs. covaiate prominence
```

## f)
The height and (by definition) the prominence of Mount Everest are both 8848 meters. Compute a prediction for the probability that an attempt at this summit will be successful. First consider the predicted value and its variance on the scale of the linear predictor (vcov gives you the estimate variance matrix of $\hat\beta$ ). Also compute a 95% confidence interval for the predicted value on this scale based on asymptotic normality of $\hat\beta$ . Transform this confidence interval to the probability scale and explain the theory behind the transformation you're using.



\newpage
# Part 2: Eliteserien 2018

In this problem we will use a generalized linear model to analyse part of the 2018 results from the Norwegian elite football league.

Each match is represented by pairs of consecutive rows in the data frame; the first row below contains the number of goals scored by Molde against Sandefjord fotball with Molde playing at their home field, and the second row contains the number of goals by Sandefjord fotball (not playing at their home field) against Molde in the same match. The covariates attach and defence will by default encoded as factors, both with 16 levels and with treatment contrasts and with BodoeGlimt by default chosen as the reference category. The variable 'home' will similarly be encoded as a factor with two levels. 

The data is incomplete in that the number of goals for some matches were not available at the time the data were recorded. Our aim is to fit a model to the available data and then simulate the remaining matches using the fitted model. Based on these simulations we will estimate the probabilities of each team winning the whole league. 

```{r}
long <- read.csv("https://www.math.ntnu.no/emner/TMA4315/2019h/eliteserie.csv")
```

## a)
```{r}
mod <- glm(goals ~ attack + defence + home, poisson, data=long)
```

The assumption of Poisson regression fits well, as the observations $y_i \in \{1, 2, \dots,32 \}$ are count data, indicating how often some event of interest has been observed in a certain period of time. The event is scoring a goal in a 90 minute football game. In a Poisson model it is assumed that the $y_i$ are independently $\text{Po}(\lambda_i)$ distributed, which is reasonable to assume. 

For this model, the response variable is number of goals per game for each team. Reading from the data, there are 16 football teams. The covariates in this model are 'attack', 'defence' and 'home', where 'attack' and 'defence' has the 16 football teams as levels, and 'home' has two levels (yes or no). 

The level 'BodoeGlimt' is chosen as the reference category, meaning that it is included in the intercept. Making 15 levels? 

Given the chosen Poisson regression, the linear predictor $\eta_i =\textbf{x}_i^T\boldsymbol{\beta}$ is connected with the rate $\lambda_i$ via 
$$\lambda_i = \text{exp}(\eta_i) = \text{exp}(\beta_0 + \beta_1x_{i1} + \dots + \beta_{32}x_{i32})$$ 
where $\lambda_i$ is the expected number of goals scored by the chosen team. This can be written in log-linear form:
$$
\ln\lambda_i = \eta_i = \beta_0 + \beta_1x_{i1} + \dots + \beta_{32}x_{i32}
$$ 
and more neatly written as 

$$
\ln\lambda_i = \mu + \alpha_{j(i)} + \beta_{k(i)} + \gamma_{h(i)}
$$
where $\mu$ is the intercept, $\alpha_{j(i)}$ is the attack coefficient for team j in observation i, $\beta_{k(i)}$ is the defence coefficient for team k $\neq$ j in observation i, and $\gamma_{h(i)}$ is the home coefficient dependent on which team is playing their home field (i.e. h $\eq$ j or h $\eq$ k). 

The parameters associated with each level of 'attack', 'defence' and 'home' say something about the increase or decrease of the log-response. 'attack' has 16 levels, where each parameter indicates if that team's attack parameter will increase or decrease the log-response if included. Same goes for 'defence', but it is a bit trickier to think about. The lower defence parameter, the better the team. That's because the Poisson model calculates the expected number of goals for one team. That means the attack parameter for that team is included, home parameter is included and the defence parameter of the opposing team is inlcuded as well. If the opposing team's defence parameter is small (or negative) it will not add much to the expected number of goals in the end (it might even subtract). That's why a higher value of attack parameter indicates more goals for the team, and a smaller value of defence parameter indicates a tougher opponent team. 

## b) 
\textit{If good teams tend to have both strong defence and strong attack capabilities, what kind of association would you expect between the above parameters? Is there any evidence of such a relationship between the parameters? You may want to use cor.test to test for this.}

If a team has strong attack capabilities, the parameter for attack of the team will be large, and if they have strong defence capabilities the defence parameter will be small (or negative). 

Correlation measures the tendency for the variables to fluctuate. The value of the correlation coefficient is between $-1$ and $+1$. In general, a positive correlation will indicate that an increase (decrease) in one variable leads to an increase in the second variable, whereas a negative correlation indicates that an increase in one variable leads to a decrease in the other. 

We calculate the correlation between the parameters attack and defence, and show the result below.

```{r}
correlation <- cor.test(mod$coefficients[2:16] , mod$coefficients[17:31] , data=long)
correlation
```

As you can see, the correlation is $-0.055$, which is a medium negative correlation value on the scale from [-1,0]. 

LIV FORKLAR PLS halp


## c) 
\textit{Based on the observed deviance, is there evidence of overdispersion in the data? What would be possible sources of over- and underdispersion in this example?}

To check for overdispersion in the data we use the trusted Chi squared test: if the deviance is larger than the Chi squared 0.05 and 352 degrees of freedom quantile, there is evidence of overdispersion in the model.

The calculations are shown below:

```{r}
d_mod2 <- summary(mod)$deviance
d_mod2
chisq_mod2 <- qchisq(1-0.05, df=352)
chisq_mod2

# Alternative method:
d_mod2 / 352 #divide deviance by df, if close to 1 --> not overdispersion
```

Since $\text{D} \ngtr \chi^2_{0.05,352}$, we conclude that there is no overdispersion in the data, and we do not need to introduce a quasipoisson model. The alternative method of dividing the deviance by the degrees of freedom is close to one, indicating that there indeed is no sign of overdispersion. 

Possible sources of over- and underdispersion in this example could be

idk

## d) 
Lag masse ting

##e)
Expected number of goals
```{r}
#predict(mod, newdata, type="response")
```

##f)
